#!/usr/bin/env python3
import torch
from torch import nn
from crop_classification.utils import read_data, TorchDataset, print_metrics
from torch.utils.data import DataLoader
from sklearn.metrics import accuracy_score


class ANN(nn.Module):
    def __init__(self, device):
        self.device = device
        super().__init__()

        self.conv1 = nn.Conv2d(1, 8, (3, 7))
        self.mlp = nn.Sequential(
            nn.Linear(96, 32),
            nn.ReLU(),
            nn.Linear(32, 7),
        ).to(self.device, dtype=torch.float32)

        self.softmax = nn.Softmax(dim=1).to(self.device, dtype=torch.float32)

    def forward(self, X):
        X = X.unsqueeze(1)
        X = nn.functional.pad(X, (3, 3, 1, 1), "circular")
        X = self.conv1(X).flatten(1, 3)

        return self.softmax(self.mlp(X))


def main():
    torch.cuda.empty_cache()
    torch.autograd.set_detect_anomaly(True)
    torch.multiprocessing.set_start_method("spawn")
    if torch.backends.mps.is_available():
        device = "mps"
    elif torch.cuda.is_available():
        device = "cuda"
    else:
        device = "cpu"
    print(device)

    X_tr, X_te, X_balanced, y_tr, y_te, y_balanced = read_data()

    train_data = TorchDataset(X_tr, y_tr)
    train_dataset, val_dataset = torch.utils.data.random_split(train_data, [0.8, 0.2])
    train_data_loader = DataLoader(
        train_dataset,
        num_workers=0,
        batch_size=64,
        shuffle=True,
        drop_last=True,
    )
    val_data_loader = DataLoader(
        val_dataset,
        batch_size=64,
        num_workers=0,
        shuffle=True,
        drop_last=True,
    )

    best_acc_of_all = 0
    best_model_of_all = None
    for alpha in [10.0, 3.0, 1.0, 0.3, 0.1]:
        print(f"Hyperparameter tuning: alpha = {alpha}")

        model = nn.DataParallel(ANN(device)).to(device, dtype=torch.float32)
        optimizer = torch.optim.SGD(model.parameters(), lr=alpha)
        # scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.8, verbose=True)
        criterion = nn.CrossEntropyLoss().to(device, dtype=torch.float32)

        best_acc = 0.0
        best_model = None
        for epoch in range(100):
            for X_batch, y_batch in train_data_loader:
                X_batch = X_batch.reshape(len(X_batch), 2, 6)
                y_hat = model((X_batch.to(device, dtype=torch.float32)))

                optimizer.zero_grad()

                loss = criterion(
                    y_hat,
                    nn.functional.one_hot(y_batch.long(), num_classes=7).to(
                        device, dtype=torch.float32
                    ),
                )
                loss.backward()
                optimizer.step()

            # scheduler.step()

            val_acc = 0.0
            val_count = 0.0
            val_loss = 0.0
            for X_val, y_val in val_data_loader:
                X_val = X_val.reshape(len(X_val), 2, 6)
                y_val_hat = model((X_val.to(device, dtype=torch.float32)))

                loss = criterion(
                    y_val_hat,
                    nn.functional.one_hot(y_val.long(), num_classes=7).to(
                        device, dtype=torch.float32
                    ),
                )

                y_val_hat = torch.argmax(y_val_hat, dim=1)

                acc = accuracy_score(y_val.cpu().numpy(), y_val_hat.cpu().numpy())

                val_loss += loss * len(X_val)
                val_acc += acc * len(X_val)
                val_count += len(X_val)

            if best_acc < val_acc / val_count:
                print(
                    f"Epoch {epoch}: Validation Loss: {val_loss / val_count} / Accuracy: {val_acc / val_count}"
                )
                print("Update model")
                best_acc = val_acc / val_count
                best_model = model

        if best_acc_of_all < best_acc:
            print(f"Hyperparameter tuning: Update at alpha = {alpha}")
            best_acc_of_all = best_acc
            best_model_of_all = model

    num_params = sum(p.numel() for p in best_model_of_all.parameters())
    print(f"Number of parameters: {num_params}")

    print("\nTest on imbalanced dataset:")
    X_te = torch.tensor(X_te)
    X_te = X_te.reshape(len(X_te), 2, 6)
    y_pred = best_model((torch.tensor(X_te).to(device, dtype=torch.float32)))
    y_pred = torch.argmax(y_pred, dim=1).cpu().numpy()
    print_metrics(y_pred, y_te)

    print("\nTest on balanced dataset:")
    X_balanced = torch.tensor(X_balanced)
    X_balanced = X_balanced.reshape(len(X_balanced), 2, 6)
    y_pred = best_model((torch.tensor(X_balanced).to(device, dtype=torch.float32)))
    y_pred = torch.argmax(y_pred, dim=1).cpu().numpy()
    print_metrics(y_pred, y_balanced)


if __name__ == "__main__":
    main()
